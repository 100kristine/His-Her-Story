{
 "metadata": {
  "name": "",
  "signature": "sha256:a49740a601fe7419273c0ea1ff2c072d921807d12730c35f432fc577ac0febae"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk\n",
      "import pickle,os\n",
      "from nltk.util import trigrams,bigrams\n",
      "import random\n",
      "from nltk.chunk import*\n",
      "from nltk.chunk.regexp import*\n",
      "from nltk.chunk.util import*\n",
      "from collections import defaultdict\n",
      "from nltk.corpus import brown\n",
      "import nltk.grammar\n",
      "from nltk.parse import generate\n",
      "from nltk import CFG\n",
      "from nltk.corpus import wordnet_ic"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 170
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%run weddingGraph.ipynb"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "files = [\"California_1-21\",\"California_22-81\",\"California_83-143\"]\n",
      "\n",
      "def loadData(files):\n",
      "    #data in format {url: textObject}\n",
      "    res = []\n",
      "    for f in files:\n",
      "        res += pickle.load(open(f,'r'))\n",
      "    d = defaultdict()\n",
      "    for result in res:\n",
      "        d[result['original_link']] = result\n",
      "    return d\n",
      "\n",
      "def tokenizeSentenceOnly(sentence):\n",
      "    return [word for word in nltk.word_tokenize(sentence) if word not in string.punctuation]\n",
      "\n",
      "def tokenizePar(par):\n",
      "    return [[word for word in nltk.word_tokenize(sent) if word not in string.punctuation] for sent in nltk.sent_tokenize(par)]\n",
      "\n",
      "def tokenize(pars):\n",
      "    #tokenize text\n",
      "    # returns cleaned [tokenized sentences without punctuation] and collapsed [no dividers between sentences]\n",
      "    strng = ' '.join(pars)\n",
      "    words = [nltk.word_tokenize(sent) for sent in nltk.sent_tokenize(strng)]\n",
      "    punct = string.punctuation\n",
      "    cleaned = []\n",
      "    collapsed = []\n",
      "    for sentence in words:\n",
      "        cleaned.append([word for word in sentence if word not in punct])\n",
      "        for word in sentence:\n",
      "            if word not in punct:\n",
      "                collapsed.append(word)\n",
      "    return cleaned,collapsed\n",
      "\n",
      "def tag(sents):\n",
      "    #tag text\n",
      "    return [nltk.pos_tag(sent) for sent in sents]\n",
      "\n",
      "def tagAll(textObjects):\n",
      "    tagged = []\n",
      "    for obj in textObjects:\n",
      "        tagged.append([tag(text) for text in obj['paragraphs']])\n",
      "    return tagged\n",
      "\n",
      "def tagNtoken(data):\n",
      "    for url in data.keys():\n",
      "        cleaned,collapsed = tokenize(data[url]['paragraphs'])\n",
      "        data[url]['tokens'] = cleaned\n",
      "        data[url]['tagged'] = tag(data[url]['tokens'])\n",
      "        data[url]['collapsed'] = collapsed\n",
      "    return data\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 70
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#data = loadData(files)\n",
      "#data = tagNtoken(data)\n",
      "\n",
      "#f = open('tagged_tokenized_data','w')\n",
      "#pickle.dump(data,f)\n",
      "#f.close()\n",
      "\n",
      "def findMostCommon(n=150):\n",
      "    if 'most_common' not in os.listdir('.'):\n",
      "        unigrams = nltk.FreqDist(pickle.load(open('allUnigrams','r'))).most_common(n)\n",
      "        pickle.dump(unigrams,open('most_common'),'w')\n",
      "    return pickle.load(open('most_common','r'))\n",
      "\n",
      "mostCommon = set([word[0] for word in findMostCommon()])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 203
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Load Data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data = pickle.load(open('tagged_tokenized_data_full_fromTheBride','r'))\n",
      "graph = pickle.load(open('superGraph','r'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#for item in data.keys():\n",
      "#    temp = [word.lower() for word in data[item][\"collapsed\"]]\n",
      "#    data[item][\"collapsed\"] = temp\n",
      "#    other = []\n",
      "#    for sentence in data[item][\"tokens\"]:\n",
      "#        other.append([word.lower() for word in sentence])\n",
      "#    data[item][\"tokens\"] = other"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Explore the Data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Modify"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def fromtheBride(dataObj):\n",
      "    #Return indices to slice out text/tokens related to \"from the bride\"\n",
      "    #Phrase is not always at the start of the sentence, due to tokenizer\n",
      "    fromBride = [None,None]\n",
      "    tokens = dataObj['tokens']\n",
      "    for i in range(len(tokens)):\n",
      "        bi = bigrams(tokens[i])\n",
      "        if ('from','the') in bi:\n",
      "            start = bi.index(('from','the'))\n",
      "            if (start + 2) < len(tokens[i]):\n",
      "                if tokens[i][start+2] == 'bride':\n",
      "                    fromBride[0] = i\n",
      "                else:\n",
      "                    fromBride[1] = i if i > 0 else None\n",
      "    if fromBride[0]:\n",
      "        if not fromBride[1]:\n",
      "            fromBride[1] = len(tokens)-1\n",
      "        return tokens[fromBride[0]:fromBride[1]]\n",
      "    else:\n",
      "        return None\n",
      "\n",
      "def modify_data(data,itemName,function):\n",
      "    #cache more information\n",
      "    for key in data.keys():\n",
      "        change = function(data[key])\n",
      "        if change != None:\n",
      "            data[key][itemName] = change\n",
      "        else:\n",
      "            data[key][itemName] = False\n",
      "    return data\n",
      "#pickle.dump(data,open('tagged_tokenized_data_lowercase','w'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## Chunking Code"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def getChunk(pattern,sentence):\n",
      "    #Get all phrases matching pattern in a single sentence\n",
      "    parser = RegexpChunkParser([ChunkRule(pattern,\"chunk\")],'phrase')\n",
      "    parsed = parser.parse(sentence)\n",
      "    return [i for i in parsed.subtrees(filter=lambda x: x.label() == 'phrase')]\n",
      "\n",
      "def collapseTree(tree,noTags=True,joinAll=False):\n",
      "    #Input: parse tree\n",
      "    #Outputs collapsed tree, set to no tags or joined (as string) \n",
      "    if noTags:\n",
      "        results = [sublst[0] for sublst in [lst for lst in tree.leaves()]]\n",
      "    else:\n",
      "        results = tree.leaves()\n",
      "    if joinAll:\n",
      "        return ' '.join(results)\n",
      "    return results\n",
      "    \n",
      "def getItems(listTrees,noTags=True,joinAll=False):\n",
      "    #Retrieve words from parse tree. Can set to not include tags\n",
      "    results = []\n",
      "    #for item in listTrees:\n",
      "    #if len(item) > 0:\n",
      "    for tree in listTrees:\n",
      "        if len(tree) > 0:\n",
      "            if noTags:\n",
      "                results.append([sublst[0] for sublst in [lst for lst in tree[0].leaves()]])\n",
      "            else:\n",
      "                results.append(tree[0].leaves())\n",
      "    if joinAll:\n",
      "        return [' '.join(result) for result in results]\n",
      "    return results"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Get Building Blocks"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "wedding = set([\"wedding\",\"marriage\",\"ceremony\",\"married\",\"marry\"])\n",
      "\n",
      "def getRelevantSentences(sample,setItems,tagged=True):\n",
      "    relevant = []\n",
      "    for sentence in sample['tokens']:\n",
      "        intersection  = setItems & set(sentence)\n",
      "        if len(intersection) > 0:\n",
      "            if tagged:\n",
      "                relevant.append(sample['tagged'][sample['tokens'].index(sentence)])\n",
      "            else:\n",
      "                relevant.append((sample['tokens'].index(sentence)))\n",
      "    return relevant\n",
      "\"\"\"\n",
      "def intent(sample):\n",
      "    #Collect all sentences that indicate some kind of intention or wish behind an action\n",
      "    #This may not be present if \"from the bride\" section is not in that post\n",
      "    #print sample[\"fromtheBride\"] != False\n",
      "    #Returns list of indices to the sentences which signify intent\n",
      "    relevant = []\n",
      "    for sentence in sample['tokens']: #so is too generic\n",
      "        intersection  = cause & set(sentence)\n",
      "        if len(intersection) > 0:\n",
      "            if \"since\" in intersection:\n",
      "                if \"ever since\" in ' '.join(sentence):\n",
      "                    continue\n",
      "            relevant.append((sample['tokens'].index(sentence)))\n",
      "    return relevant\n",
      "\"\"\"\n",
      "def getObjectOfIntent(sample):\n",
      "    indices = intent(sample)\n",
      "    #sentences = [sample['tokens'][i] for i in indices]\n",
      "    pattern = \"<PRP><VBD><TO>?<.*>*\" #Look behinds don't work?\n",
      "    results = []\n",
      "    for index in indices:\n",
      "        print  ' '.join(sample['collapsed'])\n",
      "        results.append(getChunk(pattern,sample['tagged'][index]))\n",
      "        print sample['tagged'][index]\n",
      "    return results#getItems(results,True,True)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Testing"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def getRandomDataPoints(data,n=5):\n",
      "    indices = [random.randint(0,len(data)) for i in range(n)]\n",
      "    keys = data.keys()\n",
      "    randData = []\n",
      "    for index in indices:\n",
      "        randData.append(data[keys[index]])\n",
      "    return randData\n",
      "\n",
      "def testFunctionRandom(data,function,prnt=False):\n",
      "    #Selects 5 or fewer random posts to test function on.\n",
      "    randData = getRandomDataPoints(data)\n",
      "    results = []\n",
      "    for point in randData:\n",
      "        if prnt:\n",
      "            print point['paragraphs']\n",
      "        results.append(function(point))\n",
      "    return results\n",
      "\n",
      "def testFunction(data,function):\n",
      "    #Tests on entire set\n",
      "    indices = [random.randint(0,len(data)) for i in range(5)]\n",
      "    keys = data.keys()\n",
      "    for index in indices:\n",
      "        print keys[index]\n",
      "        function(data[keys[index]])\n",
      "    return"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Features"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Theme"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "##Helpers\n",
      "\n",
      "\n",
      "def getWantedPortion(verbs,taggedSentence):\n",
      "    for i in range(len(taggedSentence)):\n",
      "        if taggedSentence[i][0] in verbs:\n",
      "            return taggedSentence[i:]\n",
      "    return []\n",
      "\n",
      "\"\"\"phrase: {<PRP><VBD><TO><.*>*}\"\"\"\n",
      "def getPhrases(expression):\n",
      "    \"\"\"Finds expressions related to inspiration for wedding. Returns as string.\"\"\"\n",
      "    parsed = nltk.RegexpParser(\"\"\"phrase: {<VBD><TO>?<.*>*}\"\"\").parse(expression)\n",
      "\n",
      "    \"\"\"phrase: {<PRP.+>*<JJ.*|NN.*|DT|CC>*<JJ.*>+<NN.*>+}\n",
      "                                          {<TO><VB><VBN><IN>}\n",
      "                                          {<JJ.*>+<CC>*<JJ.*>*<JJ.*|NN.*>}\n",
      "                                          {<DT>+<NN|JJ>+<DT|NN|IN|JJ|CC>+<NN.*>+<VBG>?}\n",
      "                                          {<JJ|NN>+<JJ|NN|CC>+<NN.*>+}\n",
      "                                          {<JJ|NN.*><NN.*|JJ><CC><JJ>*<NN.*>}\n",
      "                                          ).parse(expression)\"\"\"\n",
      "    \n",
      "    results = [i.leaves() for i in parsed.subtrees(filter=lambda x: x.label() == 'phrase')]\n",
      "    return [' '.join([r[0] for r in res]) for res in results]\n",
      "\n",
      "#print getWantedPortion(cause,ex1)\n",
      "\n",
      "def causeTests():\n",
      "    ex1 = nltk.pos_tag('we wanted our wedding to incorporate our favorite things so we peppered little touches in the details'.split(' '))\n",
      "    ex2 = nltk.pos_tag('ever since i was a little girl i always wanted to get married outside'.split(' '))\n",
      "\n",
      "    ex3 = nltk.pos_tag('I wanted the look to be elegant classic and timeless but also contemporary and fresh'.split(' '))\n",
      "    ex4 = nltk.pos_tag('I wanted the inside of the reception room to be a white wonderland with flowers galore and the furniture to be sleek white and contemporary with a lounge feel'.split(' '))\n",
      "    ex5 = nltk.pos_tag(\"tara wanted the evening to reflect her laid-back style\".split(\" \"))\n",
      "    ex6 = nltk.pos_tag(\"my goal was to have a wedding that reflected my personal style\".split(\" \"))\n",
      "\n",
      "    print getPhrases(getWantedPortion(cause,ex1))\n",
      "    print getPhrases(getWantedPortion(cause,ex2))\n",
      "    print getPhrases(getWantedPortion(cause,ex3))\n",
      "    print getPhrases(getWantedPortion(cause,ex4))\n",
      "    print getPhrases(getWantedPortion(cause,ex5))\n",
      "    print getPhrases(getWantedPortion(cause,ex6))\n",
      "    return\n",
      "\n",
      "def getWeddingInspiration(sample):\n",
      "    cause = set([\"desired\",\"sought\",\"wanted\",\"intended\",\"wished\",\"chose\"])\n",
      "    relevant = [getWantedPortion(cause,sentence) for sentence in getRelevantSentences(sample,cause,tagged=True)]\n",
      "    #print relevant\n",
      "    #if len(relevant) == 0:\n",
      "    #    print sample['paragraphs']\n",
      "    return [item[0] for item in [getPhrases(sentence) for sentence in relevant] if len(item) >0]\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 32
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import string\n",
      "\n",
      "def extract(pattern,tokens):\n",
      "    trees = [tree.leaves() for tree in getChunk(pattern,tokens)]\n",
      "    return [' '.join([sub[0] for sub in tree]) for tree in trees]\n",
      "    \n",
      "def parseIntro(sample):\n",
      "    #Curator usually also characterizes wedding\n",
      "    tokens = nltk.pos_tag(tokenizeSentenceOnly(sample['paragraphs'][0]))\n",
      "    results = extract(\"<JJ>+<JJ|DT>*<NN|NNS>+\",tokens)\n",
      "    bad = set([\"gallery\",\"idea\",\"bride\",\"sense\",\"photo\",\"thing\",\"photoshoot\", \"t\",\"s\",\"good\"])\n",
      "    return filter(lambda x:len(set(x.split(' '))&bad)==0,results)\n",
      "\n",
      "\n",
      "def getKeyphrases(text,stub=False):\n",
      "    results = []\n",
      "    for sentence in text:#sample[\"tagged\"]:\n",
      "        if stub:\n",
      "            results += extract(\"<JJ>*<NN|NNS>+\",sentence)\n",
      "        else:\n",
      "            results += extract(\"<JJ>+<JJ>*<NN|NNS>+\",sentence)\n",
      "    bad = set(['s','t'])\n",
      "    return filter(lambda x:len(set(x.split(' '))&bad)==0,results)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 101
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#testFunctionRandom(data,parseIntro)\n",
      "pts = getRandomDataPoints(data,100)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 280
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pts.keys()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "AttributeError",
       "evalue": "'list' object has no attribute 'keys'",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-284-8ada62ca7a96>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'keys'"
       ]
      }
     ],
     "prompt_number": 284
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def printSynsets(word):\n",
      "    #See what synsets exist, manually select interesting ones\n",
      "    for syn in wn.synsets(word):\n",
      "        print syn.name(), syn.definition() \n",
      "    return\n",
      "\n",
      "def mineStory(sample,items=met):\n",
      "    met = set([\"met\"])\n",
      "    proposed = (\"proposed\",\"asked me to marry\",\"popped the question\")\n",
      "    \n",
      "    pars = getRelevantParagraph(sample,met)\n",
      "    pars2 = getRelevantParagraph(sample,proposed)\n",
      "    \n",
      "    phrases = [getWantedPortion(met,sent) for sent in getRelevantSentences(sample,met,tagged=True)]\n",
      "    \n",
      "    #print \"relevant\",len(phrases)\n",
      "    \n",
      "    return [ ' '.join([word[0] for word in phrase[1:]]) for phrase in phrases],pars,pars2 \n",
      "\n",
      "    \n",
      "    \n",
      "    #keyphrases = [item.split(' ') for item in getKeyphrases(phrases,True)]\n",
      "    #return cleanWords(keyphrases,mostCommon)\n",
      "\n",
      "def cleanWords(keyphrases,mostCommon):\n",
      "    cleaned = []\n",
      "    ok = set(['friend','friends','family','Chicago'])\n",
      "    for item in keyphrases:\n",
      "        if len(item) > 1:\n",
      "            if len([i for i in item if i in mostCommon and i not in ok]) == 0:\n",
      "                cleaned.append(' '.join(item))\n",
      "        else:\n",
      "            if item[0] not in mostCommon:\n",
      "                cleaned.append(item[0])\n",
      "    return cleaned\n",
      "\n",
      "\n",
      "def getRelevantParagraph(sample,phrases):\n",
      "    pars = []\n",
      "    for par in sample['paragraphs']:\n",
      "        for phrase in phrases:\n",
      "            if phrase in par and par not in pars:\n",
      "                pars.append(par)\n",
      "                continue\n",
      "    return pars\n",
      "\n",
      "def findAllDataPts_Story(data):\n",
      "    num = 0\n",
      "    partiallyProcessed = []\n",
      "    for key in data.keys():\n",
      "        pt = data[key]\n",
      "        item = {}\n",
      "        item['name'] = key\n",
      "        item['inspiraton'] = getWeddingInspiration(pt)\n",
      "        item['intro'] = parseIntro(pt)\n",
      "        item['keyphrases'] = getKeyphrases(pt['tagged'])\n",
      "        item['met'],item['metPars'],item['propsedPars'] =  mineStory(pt)\n",
      "        if len(item['met']) != 0:\n",
      "            num +=1\n",
      "            partiallyProcessed.append(item)\n",
      "        else:\n",
      "            del[item]\n",
      "            #print \"Inspirations and Desires: \", insp\n",
      "            #print \"Blogger's Description: \", intro\n",
      "            #print \"Keyphrases: \", tagged\n",
      "            #print \"Met:\", met\n",
      "            #print \"\\n\"\n",
      "        if num%10==0:\n",
      "            print num\n",
      "    print num\n",
      "    pickle.dump(partiallyProcessed,open('data_with_metStories','w'))\n",
      "    return"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 291
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 246
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.corpus import wordnet as wn"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 104
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 282
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "place = wn.synset('place.n.03').lch_similarity(wn.synsets('college',)[2])\n",
      "#place.path_similarity()\n",
      "\n",
      "place\n",
      "printSynsets('college')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "college.n.01 the body of faculty and students of a college\n",
        "college.n.02 an institution of higher education created to educate and grant degrees; often a part of a university\n",
        "college.n.03 a complex of buildings in which an institution of higher education is housed\n"
       ]
      }
     ],
     "prompt_number": 249
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def checkWord(word):\n",
      "    return wn.synsets(word)\n",
      "checkWord('college')\n",
      "\n",
      "def getTimeTrigrams(par):\n",
      "    if years in par:\n",
      "        tri = trigrams(par)\n",
      "    return #check for year in the middle (years old, years ago, years )\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 153
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def searchSentence(sent,verb,indicators=1):\n",
      "    place = set(['at','in','by'])\n",
      "    time = set(['when'])\n",
      "    for i in range(len(sent)):\n",
      "        if verb in sent[i] and i+indicators < len(sent):\n",
      "            ind = sent[i+indicators]\n",
      "            if ind in place:\n",
      "                searchPlaces()\n",
      "            elif ind in time:\n",
      "                searchTimes()\n",
      "    return\n",
      "\n",
      "searchSentence(tagged[0],\"met\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[('at', 'IN'), ('a', 'DT')]\n"
       ]
      }
     ],
     "prompt_number": 79
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}