{
 "metadata": {
  "name": "",
  "signature": "sha256:15e3cc00d5b1c1bd65a0dbdb74bbf8c00cc7cfebcaf606b28c7c9891755001a4"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk,pickle\n",
      "from nltk.corpus import wordnet as wn\n",
      "from collections import defaultdict\n",
      "import string\n",
      "from nltk.util import bigrams"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "files = [\"California_1-21\",\"California_22-81\",\"California_83-143\",\"Destination_1-41\",\n",
      "         \"Destination_42-82\",\"Destination_83-123\",\"Midwest_1-41\",\"Midwest_42-82\",\"Midwest_83-123\"]\n",
      "\n",
      "def loadData(files):\n",
      "    #data in format {url: textObject}\n",
      "    res = []\n",
      "    for f in files:\n",
      "        res += pickle.load(open(f,'r'))\n",
      "        \n",
      "    d = defaultdict()\n",
      "    for result in res:\n",
      "        d[result['original_link']] = result\n",
      "    return d\n",
      "\n",
      "data = loadData(files)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sent_tokenizer=nltk.data.load('tokenizers/punkt/english.pickle')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def tokenize(pars):\n",
      "    #tokenize text\n",
      "    # returns cleaned [tokenized sentences without punctuation] and collapsed [no dividers between sentences]\n",
      "    strng = ' '.join(pars)\n",
      "    words = [nltk.word_tokenize(sent) for sent in nltk.sent_tokenize(strng)]\n",
      "    punct = string.punctuation\n",
      "    cleaned = []\n",
      "    collapsed = []\n",
      "    for sentence in words:\n",
      "        cleaned.append([word for word in sentence if word not in punct])\n",
      "        for word in sentence:\n",
      "            if word not in punct:\n",
      "                collapsed.append(word)\n",
      "    return cleaned,collapsed\n",
      "\n",
      "def tag(sents):\n",
      "    #tag text\n",
      "    return [nltk.pos_tag(sent) for sent in sents]\n",
      "\n",
      "def tagAll(textObjects):\n",
      "    tagged = []\n",
      "    for obj in textObjects:\n",
      "        tagged.append([tag(text) for text in obj['paragraphs']])\n",
      "    return tagged\n",
      "\n",
      "def tagNtoken(data):\n",
      "    count = 1\n",
      "    for url in data.keys():\n",
      "        if count%50 == 0:\n",
      "            print count, \" done\"\n",
      "        if \"destination\" in url:\n",
      "            data[url][\"paragraphs\"] = data[url][\"paragraphs\"][:-9]\n",
      "        \n",
      "        cleaned,collapsed = tokenize(data[url]['paragraphs'])\n",
      "        data[url]['tokens'] = cleaned\n",
      "        data[url]['tagged'] = tag(data[url]['tokens'])\n",
      "        data[url]['collapsed'] =  [word.lower() for word in collapsed]\n",
      "        \n",
      "        #Change everything to lowercase\n",
      "        other = []\n",
      "        for sentence in data[url][\"tokens\"]:\n",
      "            other.append([word.lower() for word in sentence])\n",
      "        data[url][\"tokens\"] = other\n",
      "        count += 1\n",
      "    return data\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def fromtheBride(dataObj):\n",
      "    #Return indices to slice out text/tokens related to \"from the bride\"\n",
      "    #Phrase is not always at the start of the sentence, due to tokenizer\n",
      "    fromBride = [None,None]\n",
      "    tokens = dataObj['tokens']\n",
      "    for i in range(len(tokens)):\n",
      "        bi = list(bigrams(tokens[i]))\n",
      "        if ('from','the') in bi:\n",
      "            start = bi.index(('from','the'))\n",
      "            if (start + 2) < len(tokens[i]):\n",
      "                if tokens[i][start+2] == 'bride':\n",
      "                    fromBride[0] = i\n",
      "                else:\n",
      "                    fromBride[1] = i if i > 0 else None\n",
      "    if fromBride[0]:\n",
      "        if not fromBride[1]:\n",
      "            fromBride[1] = len(tokens)-1\n",
      "        return tokens[fromBride[0]:fromBride[1]]\n",
      "    else:\n",
      "        return None\n",
      "\n",
      "def modify_data(data,itemName,function):\n",
      "    #cache more information\n",
      "    for key in data.keys():\n",
      "        change = function(data[key])\n",
      "        if change != None:\n",
      "            data[key][itemName] = change\n",
      "        else:\n",
      "            data[key][itemName] = False\n",
      "    return data"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data = tagNtoken(data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "50  done\n",
        "100"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "  done\n",
        "150"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "  done\n",
        "200"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "  done\n",
        "250"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "  done\n",
        "300"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "  done\n",
        "350"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "  done\n",
        "400"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "  done\n",
        "450"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "  done\n",
        "500"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "  done\n",
        "550"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "  done\n",
        "600"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "  done\n",
        "650"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "  done\n",
        "700"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "  done\n",
        "750"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "  done\n",
        "800"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "  done\n",
        "850"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "  done\n",
        "900"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "  done\n",
        "950"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "  done\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data2 = modify_data(data,\"fromtheBride\",fromtheBride)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "f = open('tagged_tokenized_data_full','w')\n",
      "pickle.dump(data,f)\n",
      "f.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "f = open('tagged_tokenized_data_full_fromTheBride','w')\n",
      "pickle.dump(data,f)\n",
      "f.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Save preprocessed text to data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data = tagNtoken(data)\n",
      "\n",
      "f = open('tagged_tokenized_data','w')\n",
      "pickle.dump(data,f)\n",
      "f.close()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 124
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Analysis"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def removePreviews(sample):\n",
      "    #Destination weddings have preview text tacked on to main\n",
      "    #Remove post load\n",
      "    for i in "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 131
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test.keys()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 133,
       "text": [
        "['tagged',\n",
        " 'tags',\n",
        " 'collapsed',\n",
        " 'tokens',\n",
        " 'paragraphs',\n",
        " 'original_link',\n",
        " 'categories']"
       ]
      }
     ],
     "prompt_number": 133
    }
   ],
   "metadata": {}
  }
 ]
}